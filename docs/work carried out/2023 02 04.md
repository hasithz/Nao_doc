## 2023 - Feb - 04

### implementing GRU network 

The idea is to develop a simple GRU network. And construct the agent. As for the basic implementation a single GRU cell network was used in Actor Critic. 

![single cell GRU model](https://github.com/hasithz/Nao_doc/blob/main/assets/images/GRU%20cell%20model.png?raw=true)

<!-- Now in here using this model, there are few key things we can see that the step count has been stabilized, but when considering the results it is clear that the model has trained on getting it to the next end of the states where it will be rewarded 25 in the end. So even though for the high players a slight improvement was captured when introduced other players, the variation on the reward plot got bigger and effect the whole model. -->

It has been observed through the use of the language model that the step count has stabilized, however, a deeper analysis of the results highlights that the model has been trained to reach the final state where it will receive a reward of 25. This focus on the final reward has led to a slight improvement in the performance of high-performing players, but it has also caused an increase in the variability of the reward plot.

The use of reward functions in reinforcement learning is critical in shaping the behavior of the model. A reward function can be defined as a scalar value that is assigned to the model for a specific state or action, with the objective of maximizing the total reward over time. In this particular language model, the reward function was designed to award 25 points for reaching the final state.

However, this approach has had some unintended consequences. The focus on the final reward has resulted in a reduced diversity of the model's behavior, as indicated by the increased variability of the reward plot. This reduced diversity could potentially impact the overall performance of the model, especially when introduced to other players.

![step](https://github.com/hasithz/Nao_doc/blob/main/assets/images/GRU%20step%202023-02-05.png?raw=true)

![q value](https://github.com/hasithz/Nao_doc/blob/main/assets/images/GRU%20reward%202023-02-05.png?raw=true)


### implementing on LSTMs

!!! notes

yet to be updated since the last LSTM model outputs were not accurate at all
need to refer the problem a bit